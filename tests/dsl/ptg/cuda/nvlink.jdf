extern "C" %{
/*
 * Copyright (c) 2019-2021 The University of Tennessee and The University
 *                         of Tennessee Research Foundation.  All rights
 *                         reserved.
 */

#include "parsec/parsec_config.h"
#include "parsec/utils/mca_param.h"

#include "parsec/data_distribution.h"
#include "parsec/data_dist/matrix/matrix.h"
#include "parsec/data_dist/matrix/two_dim_rectangle_cyclic.h"

#include <assert.h>
#include <stdarg.h>
#include <sys/time.h>
#if defined(PARSEC_HAVE_MPI)
#include <mpi.h>
#endif  /* defined(PARSEC_HAVE_MPI) */
#if defined(PARSEC_HAVE_CUDA)
#include "parsec/mca/device/device.h"
#include <cublas_v2.h>
#endif  /* defined(PARSEC_HAVE_CUDA) */

/**
 *
 */

typedef cublasStatus_t (*cublas_dgemm_v2_t) ( cublasHandle_t handle,
                                              cublasOperation_t transa, cublasOperation_t transb,
                                              int m, int n, int k,
                                              const double *alpha,
                                              const double *A, int lda,
                                              const double *B, int ldb,
                                              const double *beta,
                                              double       *C, int ldc);

%}

%option no_taskpool_instance = true  /* can be anything */

/*
 * Globals
 */
descA             [type = "parsec_matrix_block_cyclic_t *"]
userM             [type = "parsec_matrix_block_cyclic_t *"]
NP                [type = "int"]
CuHI              [type = "parsec_info_id_t"]
NGPUs             [type = "int"]
cuda_device_index [ type = "int *" ]

/**************************************************
 *        C Creation and destruction              *
 **************************************************/
MAKE_C(g, r)

// Execution space
g = 0 .. NGPUs-1
r = 0 .. NP-1

// Parallel partitioning
: descA(0, r)

WRITE C <- NEW
        -> C GEMM1(0, g, r)

BODY
    memset(C, 0, sizeof(double)*descA->super.mt*descA->super.nt);
    parsec_advise_data_on_device(_f_C->original,
                                 cuda_device_index[g],
                                 PARSEC_DEV_DATA_ADVICE_PREFERRED_DEVICE);
END

DISCARD_C(g, r)

// Execution space
g = 0 .. NGPUs-1
r = 0 .. NP-1

// Parallel partitioning
: descA(0, r)

READ C <- C GEMM1(descA->super.mt-1, g, r)

BODY

END

/**************************************************
 *                 Data Access                    *
 **************************************************/

READ_A(m, r)

// Execution space
m = 0 .. descA->super.mt-1
r = 0 .. NP-1

// Parallel partitioning
: descA(m, r)

READ A <- descA(m, r)
       -> A GEMM1(m, 0, r)
       -> A GEMM2(m, 0, r)

BODY

END

    
/**************************************************
 *                       GEMMs                    *
 **************************************************/
GEMM1(m, g, r)

// Execution space
m = 0 .. descA->super.mt-1
g = 0 .. NGPUs-1
r = 0 .. NP-1

// Parallel partitioning
: descA(m, r)

// Parameters
READ A <- (g == 0) ? A READ_A(m, r) : A GEMM1(m, g-1, r)
       -> ((g + 1) < NGPUs)         ? A GEMM1(m, g+1, r)
RW   C <- (m == 0) ? C MAKE_C(g, r) : C GEMM1(m-1, g, r)
       -> ((m + 1) < (descA->super.mt)) ? C GEMM1(m+1, g, r)
                                    : C DISCARD_C(g, r)

BODY [type=CUDA
      dyld=cublasDgemm_v2 dyldtype=cublas_dgemm_v2_t
      weight=(1)]
{
    cublasStatus_t status;
    cublasHandle_t handle;
    double alpha=0.0;
    double beta=1.0;
    handle = parsec_info_get(&gpu_stream->infos, CuHI);
    assert(NULL != handle);
    status = parsec_body.dyld_fn( handle,
                                  CUBLAS_OP_N, CUBLAS_OP_N, 
                                  descA->super.mb, descA->super.nb, descA->super.mb,
                                  &alpha, (double*)A, descA->super.mb,
                                  (double*)A, descA->super.mb,
                                  &beta, (double*)C, descA->super.mb );
    PARSEC_CUDA_CHECK_ERROR( "cublasDgemm_v2 ", status,
                            {return -1;} );
}
END

BODY
{
    fprintf(stderr, "Kernel GEMM(%d, %d, %d) in nvlink test is running on a CPU, which is not the intended behavior\n",
            m, g, r);
}
END


GEMM2(m, g, r)

// Execution space
m = 0 .. descA->super.mt-1
g = 0 .. NGPUs-1
r = 0 .. NP-1

// Parallel partitioning
: userM(g, r)

// Parameters
READ A <- (g == 0) ? A READ_A(m, r) : A GEMM2(m, g-1, r)
       -> ((g + 1) < NGPUs)         ? A GEMM2(m, g+1, r)
RW   C <- (m == 0) ? userM(g, r)    : C GEMM2(m-1, g, r)
       -> ((m + 1) < (descA->super.mt)) ? C GEMM2(m+1, g, r)

BODY [type=CUDA
      dyld=cublasDgemm_v2 dyldtype=cublas_dgemm_v2_t
      weight=(1)]
{
    cublasStatus_t status;
    cublasHandle_t handle;
    double alpha=0.0;
    double beta=1.0;
    handle = parsec_info_get(&gpu_stream->infos, CuHI);
    assert(NULL != handle);
    status = parsec_body.dyld_fn( handle,
                                  CUBLAS_OP_N, CUBLAS_OP_N, 
                                  descA->super.mb, descA->super.nb, descA->super.mb,
                                  &alpha, (double*)A, descA->super.mb,
                                  (double*)A, descA->super.mb,
                                  &beta, (double*)C, descA->super.mb );
    PARSEC_CUDA_CHECK_ERROR( "cublasDgemm_v2 ", status,
                            {return -1;} );
}
END

BODY
{
    fprintf(stderr, "Kernel GEMM2(%d, %d, %d) in nvlink test is running on a CPU, which is not the intended behavior\n",
            m, g, r);
}
END


