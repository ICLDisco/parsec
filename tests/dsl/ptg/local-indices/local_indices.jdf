extern "C" %{
/*
 * Copyright (c) 2019-2023 The University of Tennessee and The University
 *                         of Tennessee Research Foundation.  All rights
 *                         reserved.
 *
 */

#include <sys/time.h>
#include <inttypes.h>
#include <string.h>
#include <stdlib.h>
#include "parsec/data_dist/matrix/two_dim_rectangle_cyclic.h"

/**
 * This test defines sparse execution domains to illustrate the
 * use of local indices in the PTG/JDF syntax.
 */
%}

descA            [type = "parsec_matrix_block_cyclic_t*"]
nb_tasks_array   [type = "int32_t *"]

STARTUP(odd, even)

odd = [ i = 0 .. %{ return 4; %} ] %{ return 2*i+1; %}
even = [ i = 0 .. 4 ] 2*i

: descA( (odd/2) % descA->super.mt, (even/2) % descA->super.nt)

READ A <- descA((odd/2) % descA->super.mt, (even/2) % descA->super.nt)
       -> [ i = 0 .. odd ] odd < 4 ? [ j = 0 .. %{ return even; %} .. 2 ] A tA(odd, even, %{ return i; %}, j/2) : [ j = 0 .. even .. 2 ] A tB(odd, even, i, j/2)

CTL  X <- [ i = 0 .. odd ] i == -1 ? X STARTUP(0, 0)  // Non-existent control dependency -- only to check conditions are followed
       -> [ i = 0 .. odd ] i == -1 ? X STARTUP(0, 0)  // Non-existent control dependency -- only to check conditions are followed
       -> Y tG(0)
       -> Z tG(0)

BODY
{
   usleep( rand() % 50000 + 50000);
   parsec_atomic_fetch_inc_int32(&nb_tasks_array[0]);
}
END

tG(zero)

zero = 0 .. 0

: descA(0, 0)

CTL Y <- [ i = 0 .. 4, j = 0 .. 4 ] i >= 0 ? X STARTUP(2*i+1, 2*j)
CTL Z <- [ i = 0 .. 4, j = 0 .. 4 ] i >= 0 ? X STARTUP(2*i+1, 2*j)  // Completely redundant dependency, just to check that generated code uses proper scope for local variable declaration

BODY
{
   assert(nb_tasks_array[0] == 25); // tG(0) can only execute after all STARTUP tasks have executed
   /* nothing */
}
END

tA(o, e, i, j)

o = [ k = 0 .. 4 ] 2*k+1
e = [ k = 0 .. 4 ] 2*k
i = 0 .. o < 4 ? o : -1
j = 0 .. e / 2

: descA(i % descA->super.mt, j % descA->super.nt)

READ A <- A STARTUP(o, e)

BODY
{
   usleep( rand() % 50000 + 50000);
   parsec_atomic_fetch_inc_int32(&nb_tasks_array[1]);
}
END

tB(o, e, i, j)

o = [ k = 0 .. 4 ] 2*k+1
e = [ k = 0 .. 4 ] 2*k
i = 6 .. o
j = 0 .. e / 2

: descA(i % descA->super.mt, j % descA->super.nt)

READ A <- A STARTUP(o, e)
        -> o == 7 && e == 0 && i == 7 && j == 0 ? [ l = 1 .. 2 ] A tC(l, 2*l .. 3*l)

BODY
{
   usleep( rand() % 50000 + 50000);
   parsec_atomic_fetch_inc_int32(&nb_tasks_array[2]);
}
END

tC(l1, l2)

l1 = 1 .. 2
l2 = 2*l1 .. 3*l1

: descA(l1 % descA->super.mt, l2 % descA->super.nt)

READ A <- A tB(7, 0, 7, 0)

BODY
{
   usleep( rand() % 50000 + 50000);
   parsec_atomic_fetch_inc_int32(&nb_tasks_array[3]);
}
END

extern "C" %{

#include <math.h>

int main( int argc, char** argv )
{
    parsec_local_indices_taskpool_t* tp;
    parsec_matrix_block_cyclic_t descA;
    parsec_arena_datatype_t adt;
    parsec_datatype_t dt;
    parsec_context_t *parsec;
    int ws = 1, mr = 0;
    int rc;
    int p = 1, c;
    int local_nb[4];
    int global_nb[4];
    int ret;

    srand( getpid() );

#ifdef PARSEC_HAVE_MPI
    {
        int provided;
        MPI_Init_thread(NULL, NULL, MPI_THREAD_SERIALIZED, &provided);
        MPI_Comm_size(MPI_COMM_WORLD, &ws);
        MPI_Comm_rank(MPI_COMM_WORLD, &mr);
        for(c = (int)sqrt(ws)+1; c > 0; c--) {
            if( (c < ws) && (ws % c) == 0 ) {
                p = c;
                break;
            }
        }
    }
#endif

    parsec = parsec_init(-1, &argc, &argv);
    if( NULL == parsec ) {
       exit(-1);
    }

    /**
     * Build the data and the arena to hold it up.
     */
    parsec_matrix_block_cyclic_init( &descA, PARSEC_MATRIX_DOUBLE, PARSEC_MATRIX_TILE,
                               mr /*rank*/,
                               100 /* mb */, 100 /* nb */,
                               100*10 /* lm */, 100*10 /* ln */,
                               0 /* i */, 0 /* j */,
                               100*10 /* m */, 100*10 /* n */,
                               p, ws/p, 1 /* sm */, 1 /* sn */,
                               0, 0);
    descA.mat = parsec_data_allocate( descA.super.nb_local_tiles *
                                      descA.super.bsiz *
                                      parsec_datadist_getsizeoftype(PARSEC_MATRIX_DOUBLE) );
    parsec_data_collection_set_key(&descA.super.super, "A");

    parsec_translate_matrix_type(PARSEC_MATRIX_DOUBLE, &dt);
    parsec_add2arena_rect(&adt, dt,
                                 descA.super.mb, descA.super.nb, descA.super.mb);
    /* Start the PaRSEC engine */
    rc = parsec_context_start(parsec);
    PARSEC_CHECK_ERROR(rc, "parsec_context_start");

    /* Heat up the engine: small tasks no priority */
    memset(local_nb, 0, 4*sizeof(int));
    tp = parsec_local_indices_new( &descA, (int32_t*)local_nb );
    assert( NULL != tp );
    tp->arenas_datatypes[PARSEC_local_indices_DEFAULT_ADT_IDX] = adt;
    rc = parsec_context_add_taskpool( parsec, (parsec_taskpool_t*)tp );
    PARSEC_CHECK_ERROR(rc, "parsec_context_add_taskpool");
    rc = parsec_context_wait(parsec);

    PARSEC_CHECK_ERROR(rc, "parsec_context_wait");

    rc = parsec_context_start(parsec);
    PARSEC_CHECK_ERROR(rc, "parsec_context_start");

    parsec_context_wait(parsec);

#ifdef PARSEC_HAVE_MPI
    MPI_Reduce(local_nb, global_nb, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
#else
    memcpy(global_nb, local_nb, 4*sizeof(int));
#endif
    ret = 0;
    if( 0 == mr ) {
        if( global_nb[0] != 25 ) {
            fprintf(stderr, "*** Test failed: expected 25 STARTUP tasks, found %d total\n", global_nb[0]);
            ret++;
        }
        if( global_nb[1] != 90 ) {
            fprintf(stderr, "*** Test failed: expected 90 tA tasks, found %d total\n", global_nb[1]);
            ret++;
        }
        if( global_nb[2] != 90 ) {
            fprintf(stderr, "*** Test failed: expected 90 tB tasks, found %d total\n", global_nb[2]);
            ret++;
        }
        if( global_nb[3] != 5 ) {
            fprintf(stderr, "*** Test failed: expected 5 tC tasks, found %d total\n", global_nb[3]);
            ret++;
        }
    }

    parsec_taskpool_free(&tp->super);

    free(descA.mat);
    parsec_del2arena( & adt );

    parsec_fini( &parsec);

#ifdef PARSEC_HAVE_MPI
    MPI_Finalize();
#endif

    return ret;
}

%}
