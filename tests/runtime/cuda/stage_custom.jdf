extern "C" %{
/*
 * Copyright (c) 2019-2024 The University of Tennessee and The University
 *                         of Tennessee Research Foundation.  All rights
 *                         reserved.
 * Copyright (c) 2024      NVIDIA Corporation.  All rights reserved.
 */

#include "parsec/parsec_config.h"
#include "parsec/utils/mca_param.h"

#include "parsec/data_distribution.h"
#include "parsec/data_dist/matrix/matrix.h"
#include "parsec/data_dist/matrix/two_dim_rectangle_cyclic.h"

#include <assert.h>
#include <stdarg.h>
#include <sys/time.h>
#include <mpi.h>
#if defined(PARSEC_HAVE_DEV_CUDA_SUPPORT)
#include "parsec/mca/device/cuda/device_cuda_internal.h"
#include <cublas.h>
#endif  /* defined(PARSEC_HAVE_DEV_CUDA_SUPPORT) */

#include "stage_custom.h"


static int
stage_stride_in(parsec_gpu_task_t *gtask,
                uint32_t flow_mask,
                parsec_gpu_exec_stream_t *gpu_stream){
    parsec_cuda_exec_stream_t *cuda_stream = (parsec_cuda_exec_stream_t *)gpu_stream;
    cudaError_t ret = 0;
    parsec_data_copy_t * copy_in;
    parsec_data_copy_t * copy_out;
    parsec_task_t *task = gtask->ec;
    parsec_device_gpu_module_t *in_elem_dev;
    parsec_tiled_matrix_t * dc;
    int elem_sz;
    int i;
    for(i = 0; i < task->task_class->nb_flows; i++){
        if(flow_mask & (1U << i)){
            copy_in = task->data[i].data_in;
            copy_out = task->data[i].data_out;
            dc = (parsec_tiled_matrix_t*)gtask->flow_dc[i];
            elem_sz = parsec_datadist_getsizeoftype(dc->mtype);
            in_elem_dev = (parsec_device_gpu_module_t*)parsec_mca_device_get( copy_in->device_index);
            if( !(in_elem_dev->super.type & PARSEC_DEV_CUDA) ) {
                /* copy width bytes heigth times, skipping pitch - width bytes every time */
                size_t dpitch = dc->mb * elem_sz;
                size_t spitch = dc->llm * elem_sz;
                size_t width  = dc->mb * elem_sz;
                size_t height = dc->nb;
                ret = (cudaError_t)cudaMemcpy2DAsync(  copy_out->device_private,
                                                       dpitch, /*dst pitch bytes*/
                                                       copy_in->device_private,
                                                       spitch, /*src pitch bytes*/
                                                       width, height,
                                                       cudaMemcpyHostToDevice,
                                                       cuda_stream->cuda_stream );
                PARSEC_CUDA_CHECK_ERROR( "cudaMemcpyAsync", ret, { return PARSEC_ERROR; } );
            }else{
                ret = (cudaError_t)cudaMemcpyAsync( copy_out->device_private,
                                                    copy_in->device_private,
                                                    copy_in->original->nb_elts,
                                                    cudaMemcpyDeviceToDevice,
                                                    cuda_stream->cuda_stream );
                PARSEC_CUDA_CHECK_ERROR( "cudaMemcpyAsync", ret, { return PARSEC_ERROR; } );
            }

        }
    }
    return PARSEC_SUCCESS;
}

static int
stage_stride_out(parsec_gpu_task_t *gtask,
                 uint32_t flow_mask,
                 parsec_gpu_exec_stream_t *gpu_stream){
    parsec_cuda_exec_stream_t *cuda_stream = (parsec_cuda_exec_stream_t*)gpu_stream;
    cudaError_t ret;
    parsec_data_copy_t * copy_in;
    parsec_data_copy_t * copy_out;
    parsec_task_t *task = gtask->ec;
    parsec_tiled_matrix_t * dc;
    int elem_sz;
    int i;
    for(i = 0; i < task->task_class->nb_flows; i++){
        if(flow_mask & (1U << i)){
            copy_in = task->data[i].data_out;
            copy_out = copy_in->original->device_copies[0];
            dc = (parsec_tiled_matrix_t*)gtask->flow_dc[i];
            elem_sz = parsec_datadist_getsizeoftype(dc->mtype);
            /* copy width bytes heigth times, skipping pitch - width bytes every time */
            size_t dpitch = dc->llm * elem_sz;
            size_t spitch = dc->mb * elem_sz;
            size_t width  = dc->mb * elem_sz;
            size_t height = dc->nb;
            ret = (cudaError_t)cudaMemcpy2DAsync(  copy_out->device_private,
                                                   dpitch, /*dst pitch bytes*/
                                                   copy_in->device_private,
                                                   spitch, /*src pitch bytes*/
                                                   width, height,
                                                   cudaMemcpyDeviceToHost,
                                                   cuda_stream->cuda_stream );
           PARSEC_CUDA_CHECK_ERROR( "cudaMemcpyAsync", ret, { return PARSEC_ERROR; } );
        }
    }
    return PARSEC_SUCCESS;
}

typedef void (*cublas_dgemm_t) ( char TRANSA, char TRANSB, int m, int n, int k,
                                 double alpha, double *d_A, int lda,
                                 double *d_B, int ldb,
                                 double beta,  double *d_C, int ldc );

/* Pre-declare function used as a property of some parameterized task */
static int64_t gemm_time_estimate(const parsec_task_t *task, parsec_device_module_t *dev);

typedef struct deallocation_helper_s {
    parsec_assignment_t m;
    parsec_assignment_t k;
    void** my_storage[3];
  } free_helper_t;

static int 
complete_batched_callback(parsec_device_gpu_module_t *dev,
                          parsec_gpu_task_t ** gpu_task,
                          parsec_gpu_exec_stream_t *gpu_stream)
{
    PARSEC_DEBUG_VERBOSE(10, parsec_debug_output, "complete_batched_callback for batched task %p on stream %s{%p}\n",
                          gpu_task, gpu_stream->name, (void*)gpu_stream);
    (void)dev; (void) gpu_task; (void)gpu_stream;
    parsec_list_item_t* output_stream_ghost = &dev->exec_stream[1]->fifo_pending->ghost_element;
    parsec_list_item_ring_merge(output_stream_ghost, &(*gpu_task)->list_item);
    free_helper_t* helper = (free_helper_t*)(((*gpu_task)->ec)->locals); (void)helper;
    //printf(" release %p and %p and %p\n", helper->my_storage[0], helper->my_storage[1], helper->my_storage[2]);
    (*gpu_task)->complete_stage = NULL;
    *gpu_task = NULL;
    return PARSEC_HOOK_RETURN_DONE;
}
%}

%option no_taskpool_instance = true  /* can be anything */

/*
 * Globals
 */


descA  [type = "parsec_tiled_matrix_t*"]
descB  [type = "parsec_tiled_matrix_t*"]
OUT    [type = "int*"]

/**************************************************
 *                       TASK_GPU                 *
 **************************************************/
TASK_GPU(m, k) [ time_estimate = gemm_time_estimate ]


m = 0 .. descA->mt-1
k = 0 .. descA->nt-1

: descA(m, k)

RW A <- descA(m, k)
     -> descA(m, k)
     -> A TASK_CHECK(m,k)

BODY [type=CUDA
      batch = true
      dyld=cublasDgemm dyldtype=cublas_dgemm_t]
{
    parsec_list_item_singleton(&gpu_task->list_item);
    parsec_list_item_t* store_back = NULL;
    if( !parsec_list_nolock_is_empty(gpu_stream->fifo_pending) ) {  /* more than one gpu_task in the ring */
        int how_many = 1;  /* we start with the current gpu_task */
        do {
            parsec_list_item_t* item = parsec_list_pop_front(gpu_stream->fifo_pending);
            parsec_list_item_singleton(item);
            parsec_gpu_task_t* task = (parsec_gpu_task_t*)item;

            if( gpu_task->ec->task_class == task->ec->task_class ) {
                /* same task class as the current one, possible to batch */
                (void)parsec_list_item_ring_push(&gpu_task->list_item, (parsec_list_item_t*)task);
                how_many++;  /* one more into the batch */
                PARSEC_DEBUG_VERBOSE(10, parsec_debug_output, "Add task %p to the %p batch on stream %s{%p}\n",
                                      task, gpu_task, gpu_stream->name, (void*)gpu_stream);
                if( 5 == how_many ) {
                    /* let's stop here for now */
                    break;
                }
            } else {
                if( NULL == store_back ) {
                    store_back = item;
                } else {
                    parsec_list_item_ring_push(store_back, item);  /* build the list of un-batcheable tasks */
                }
            }
        } while( !parsec_list_nolock_is_empty(gpu_stream->fifo_pending) );
        /* we now have two separated task rings: the gpu_task with all the tasks that will be batched
         * and the ring that has all the remaining items in the list (including the list's ghost_elem).
         * The remaining list is already stored in the gpu_stream->fifo_pending.
         */
        if( how_many > 1 )
            gpu_task->complete_stage = complete_batched_callback;
        PARSEC_DEBUG_VERBOSE(10, parsec_debug_output, "submit multiple tasks into one %p on stream %s{%p}\n",
                              gpu_task, gpu_stream->name, (void*)gpu_stream);
        if( NULL != store_back ) {
            parsec_list_item_ring_merge(&gpu_stream->fifo_pending->ghost_element, store_back);
        }
    }
    double lalpha = 1.0;
    double lbeta  = 2.0;
    int tempmm = descA->mb;
    int ldam   = descA->mb;

    cublasStatus_t status;
    cublasSetKernelStream( parsec_body.stream );

    parsec_gpu_task_t* current_gpu_task = gpu_task;
    free_helper_t* helper = (free_helper_t*)((gpu_task->ec)->locals);
    helper->my_storage[0] = (void*)(uintptr_t)0xdeadbeef;
    helper->my_storage[1] = (void*)(uintptr_t)0xabcdabcd;
    helper->my_storage[2] = (void*)(uintptr_t)0xcdefcdef;
    do {
        __parsec_stage_custom_TASK_GPU_task_t *task = (__parsec_stage_custom_TASK_GPU_task_t*)current_gpu_task->ec;
        _f_A = task->data._f_A.data_out;
        A = PARSEC_DATA_COPY_GET_PTR(_f_A);
        parsec_body.dyld_fn(  'N', 'N',
                            tempmm, tempmm, tempmm,
                            lalpha, (double*)A, ldam,
                                    (double*)A, ldam,
                            lbeta,  (double*)A, ldam );
        status = cublasGetError();
        PARSEC_CUDA_CHECK_ERROR( "cublasDgemm", status,
                                {return PARSEC_HOOK_RETURN_ERROR;} );
        current_gpu_task = (parsec_gpu_task_t*)current_gpu_task->list_item.list_next;
    } while( current_gpu_task != gpu_task );
}
END

/**************************************************
 *                TASK_GPU_CUSTOM_STAGE                 *
 **************************************************/
TASK_GPU_CUSTOM_STAGE(m, k) [ time_estimate = gemm_time_estimate ]


m = 0 .. descB->mt-1
k = 0 .. descB->nt-1

: descB(m, k)

RW B <- descB(m, k)
     -> descB(m, k)
     -> B TASK_CHECK(m,k)

BODY [type=CUDA
      stage_in=stage_stride_in
      stage_out=stage_stride_out
      /* B device default size will already be this one*/
      B.size=%{return descB->mb*descB->nb*parsec_datadist_getsizeoftype(descB->mtype);%}
      B.dc=descB
      dyld=cublasDgemm dyldtype=cublas_dgemm_t]
{
    double lalpha = 1.0;
    double lbeta  = 2.0;
    int tempmm = descB->mb;
    int ldbm   = descB->mb;

    cublasStatus_t status;
    cublasSetKernelStream( parsec_body.stream );
    parsec_body.dyld_fn(  'N', 'N',
                         tempmm, tempmm, tempmm,
                         lalpha, (double*)B, ldbm,
                                 (double*)B, ldbm,
                         lbeta,  (double*)B, ldbm );
    status = cublasGetError();
    PARSEC_CUDA_CHECK_ERROR( "cublasDgemm", status,
                            {return PARSEC_HOOK_RETURN_ERROR;} );

}
END

/**************************************************
 *                       TASK_CHECK                 *
 **************************************************/
TASK_CHECK(m, k)


m = 0 .. descA->mt-1
k = 0 .. descA->nt-1

: descA(m, k)

READ A <- A TASK_GPU(m, k)
READ B <- B TASK_GPU_CUSTOM_STAGE(m, k)

BODY
{
    int i,j;
    int ldam = descB->mb;
    int ldbm = descB->llm;
    for(j=0; j<descA->nb; j++) {
        for(i=0; i<descA->mb; i++) {
            /*printf("X(%d,%d) A %f B %f\n", j, i, ((double*)A)[j*ldam + i], ((double*)B)[j*ldbm + i]);*/
            if( (((double*)A)[j*ldam + i] - ((double*)B)[j*ldbm + i]) > 10e-15 ){
                (*OUT)++; break;
            }
        }
    }
}
END



extern "C" %{

static int64_t gemm_time_estimate(const parsec_task_t *task, parsec_device_module_t *dev)
{
  const parsec_stage_custom_taskpool_t *tp = (parsec_stage_custom_taskpool_t *)task->taskpool;
  int64_t mb = ((parsec_tiled_matrix_t*)tp->_g_descA)->mb;
  int64_t flops = 2 * mb * mb * mb;
  return flops / dev->gflops_fp64;
}

parsec_taskpool_t* testing_stage_custom_New( parsec_context_t *ctx, int M, int N, int MB, int NB, int P, int *ret)
{
    parsec_stage_custom_taskpool_t* testing_handle = NULL;
    int KP = 1;
    int KQ = 1;

    parsec_matrix_block_cyclic_t *descA;
    descA = (parsec_matrix_block_cyclic_t*)calloc(1, sizeof(parsec_matrix_block_cyclic_t));
    parsec_matrix_block_cyclic_init(descA, PARSEC_MATRIX_DOUBLE, PARSEC_MATRIX_TILE,
                              ctx->my_rank, MB, NB, M, N, 0, 0,
                              M, N, P, ctx->nb_nodes/P, KP, KQ, 0, 0);

    int nelems = descA->super.nb_local_tiles * descA->super.bsiz;

    descA->mat = parsec_data_allocate( (size_t) nelems *
                                       (size_t)parsec_datadist_getsizeoftype(descA->super.mtype));
    parsec_data_collection_set_key((parsec_data_collection_t*)descA, "descA");

    parsec_matrix_block_cyclic_t *descB;
    descB = (parsec_matrix_block_cyclic_t*)calloc(1, sizeof(parsec_matrix_block_cyclic_t));
    parsec_matrix_block_cyclic_init(descB, PARSEC_MATRIX_DOUBLE, PARSEC_MATRIX_LAPACK,
                              ctx->my_rank, MB, NB, M, N, 0, 0,
                              M, N, P, ctx->nb_nodes/P, KP, KQ, 0, 0);
    descB->mat = parsec_data_allocate( (size_t) nelems *
                                       (size_t)parsec_datadist_getsizeoftype(descB->super.mtype));
    assert(NULL != descB->mat);
    parsec_data_collection_set_key((parsec_data_collection_t*)descB, "descB");

    int ldam = descB->super.mb;
    int ldbm = descB->super.llm;

    int m, n;
    int count = 1;
    for(m=0; m< descB->nb_elem_r; m++){
        for(n=0; n< descB->nb_elem_c; n++){
            int start_block_lapack = m*descB->super.mb  + n*descB->super.nb*ldbm;
            int start_block_tile   = m*descA->super.bsiz + n*descA->super.bsiz*descA->nb_elem_r;
            int ii, jj;
            for(jj=0; jj<descB->super.nb; jj++) {
                for(ii=0; ii<descB->super.mb; ii++) {
                    int ind_lapack = start_block_lapack + jj*ldbm + ii;
                    int ind_tile   = start_block_tile   + jj*ldam + ii;
                    ((double*)descB->mat)[ind_lapack] = ((double*)descA->mat)[ind_tile] = count;
                    count++;
                }
            }
        }
    }

//    int i;
//    printf("INI B %p\n", descB->mat);
//    for(i = 0; i < nelems ; i++){
//        printf("INI(%d) A %p %f B %p %f\n", i,
//            &((double*)descA->mat)[i], ((double*)descA->mat)[i],
//            &((double*)descB->mat)[i], ((double*)descB->mat)[i]);
//    }

    testing_handle = parsec_stage_custom_new((parsec_tiled_matrix_t*)descA, (parsec_tiled_matrix_t*)descB, ret);

    return &testing_handle->super;
}

static void
__parsec_taskpool_stage_custom_destructor(parsec_stage_custom_taskpool_t *stage_custom_taskpool)
{
    parsec_matrix_block_cyclic_t *descA = (parsec_matrix_block_cyclic_t*)stage_custom_taskpool->_g_descA;
    parsec_matrix_block_cyclic_t *descB = (parsec_matrix_block_cyclic_t*)stage_custom_taskpool->_g_descB;

    parsec_data_free(descA->mat);
    parsec_tiled_matrix_destroy( (parsec_tiled_matrix_t*)stage_custom_taskpool->_g_descA );
    parsec_data_free(descB->mat);
    parsec_tiled_matrix_destroy( (parsec_tiled_matrix_t*)stage_custom_taskpool->_g_descB );
    free(descA);
    free(descB);
}

PARSEC_OBJ_CLASS_INSTANCE(parsec_stage_custom_taskpool_t, parsec_taskpool_t,
                          NULL, __parsec_taskpool_stage_custom_destructor);
%}
