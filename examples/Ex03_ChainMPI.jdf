extern "C" %{

/**
 * This example shows how to distribute the tasks over different nodes
 *    descriptor as global
 *    rank_of() / possibility of playing with value in rank_of
 */

#include "parsec.h"

%}

/**
 * Data descriptor used in the jdf can be declared manually as global to fix the
 * parameters order of the _New function
 */
taskdist  [ type="parsec_data_collection_t*" ]
NB        [ type="int" ]

Task(k)

k = 0 .. NB

: taskdist( k )

RW  A <- (k == 0) ? NEW : A Task( k-1 )
      -> (k < NB) ? A Task( k+1 )

BODY
{
    int *Aint = (int*)A;
    int rank;
    if ( k == 0 ) {
        *Aint = 0;
    } else {
        *Aint += 1;
    }

    MPI_Comm_rank( MPI_COMM_WORLD, &rank );
    printf("I am element %d in the chain computed on node %d\n", *Aint, rank );
}
END

extern "C" %{

/**
 * The rank_of function must return the rank owner of the task/data with the
 * coordinates given as the va_args.
 * The coordinates are given as integers, and the number of dimensions is up to
 * the user.
 *
 * In this example, we implemented a 1D function that distributes the indexes in
 * a round-robin fashion among all the nodes.
 */
static uint32_t
rank_of(parsec_data_collection_t *desc, ...)
{
    int k;
    va_list ap;

    va_start(ap, desc);
    k = va_arg(ap, int);
    va_end(ap);

    return k % desc->nodes;
}

/**
 * The vpid_of function must return the virtual process id on which the task/data with the
 * coordinates given as the va_args can run. The VPID must be a number between 0 and
 * the number of virtual processes-1. We don't use virtual processes in this example,
 * so there is only one, and we return 0.
 */
static int32_t
vpid_of(parsec_data_collection_t *desc, ...)
{
    (void)desc;
    return 0;
}

/**
 * The data_key function must return a unique identifier for the data.
 * This is needed for tracing and debugging purpose, but it's always good practice to
 * define it.
 */
static parsec_data_key_t
data_key(parsec_data_collection_t *desc, ...)
{
    int k;
    va_list ap;
    (void)desc;
    va_start(ap, desc);
    k = va_arg(ap, int);
    va_end(ap);
    return (uint64_t)k;
}

int main(int argc, char *argv[])
{
    parsec_context_t* parsec;
    int rc;
    int rank, world;
    parsec_data_collection_t taskdist;
    parsec_Ex03_ChainMPI_taskpool_t *tp;

#if defined(PARSEC_HAVE_MPI)
    {
        int provided;
        MPI_Init_thread(&argc, &argv, MPI_THREAD_SERIALIZED, &provided);
    }
    MPI_Comm_size(MPI_COMM_WORLD, &world);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
#else
    world = 1;
    rank = 0;
#endif

    parsec = parsec_init(-1, &argc, &argv);

    /**
     * Let's initialize the task distribution descriptor
     * rank_of, and vpid_of will be explained later, but right now those two
     * functions always return 0.
     */
    parsec_data_collection_init(&taskdist, world, rank);
    taskdist.rank_of = rank_of;
    taskdist.vpid_of = vpid_of;
    taskdist.data_key = data_key;

    tp = parsec_Ex03_ChainMPI_new(&taskdist, 20);

    /**
     * The arena is now also used to describe the layout to the communication
     * engine (MPI)
     */
    parsec_arena_construct(tp->arenas[PARSEC_Ex03_ChainMPI_DEFAULT_ARENA],
                           sizeof(int), PARSEC_ARENA_ALIGNMENT_SSE,
                           parsec_datatype_int_t );

    rc = parsec_context_add_taskpool( parsec, (parsec_taskpool_t*)tp );
    PARSEC_CHECK_ERROR(rc, "parsec_context_add_taskpool");
    rc = parsec_context_start(parsec);
    PARSEC_CHECK_ERROR(rc, "parsec_context_start");
    rc = parsec_context_wait(parsec);
    PARSEC_CHECK_ERROR(rc, "parsec_context_wait");

    parsec_taskpool_free((parsec_taskpool_t*)tp);

    /**
     * Cleanup the descriptor
     */
    parsec_data_collection_destroy(&taskdist);

    parsec_fini(&parsec);
#if defined(PARSEC_HAVE_MPI)
    MPI_Finalize();
#endif

    return 0;
}

%}
