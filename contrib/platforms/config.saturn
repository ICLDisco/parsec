#!/bin/bash

if [ "x$USER" = "xbosilca" ]; then
#  CUDA_DIR="/opt/cuda-5.0.7/cuda"
#  HWLOC_DIR="/home/bosilca/opt/"
#  MPI_DIR="/home/bosilca/opt/mpi"
  USER_OPTIONS+=" -DDPLASMA_PRECISIONS=d"

elif [ "x${USER}" = "xmfaverge" ]; then
  # if [ "x${CC}" = "xgcc" ]; then
  #     DSPARSE="-DPARSEC_WITH_SPARSE=ON -DPASTIX_DIR=/home/mfaverge/opt/pastix-dbg -DSCOTCH_DIR=/opt/scotch-gf"
  # else
  #     DSPARSE="-DPARSEC_WITH_SPARSE=ON -DPASTIX_DIR=/home/mfaverge/opt/pastix-intel -DSCOTCH_DIR=/opt/scotch"
  # fi
  #USER_OPTIONS+=" -DPARSEC_DIST_WITH_MPI=OFF"
  USER_OPTIONS+=" -DPARSEC_GPU_WITH_CUDA=ON"
  USER_OPTIONS+=" -DDPLASMA_PRECISIONS=d"
elif [ "x$USER" = "xrhoque" ]; then
  module load gcc
  CC=gcc
  MPI_DIR="/sw/openmpi/2.0.2/"
elif [ "x$USER" = "xdgenet" ]; then
  CC=gcc
  CXX=g++
  FC=gfortran
  MPI_DIR="/sw/openmpi/2.0.2/"
  USER_OPTIONS+=" -DDPLASMA_PRECISIONS=d"
elif [ "x$USER" = "xYOURSELF" ]; then
  PLASMA_DIR="SOMEWHERE"
  USER_OPTIONS+="-DPARSEC_OPTION=ON"
fi

echo "### Defaults for Dancer machine (intel-NV Linux cluster)"
# Do not set the compiler flags (CC, CXX, FC) and the MPI flags (MPI_DIR)
# CUDA (detected via nvcc), PAPI (detected via papi_avail), HWLOC (detected
# via hwloc-ls) nor the PYTHON_EXECUTABLE
# if you want the default detection to work.
INSTALL_PREFIX=${INSTALL_PREFIX:="$PWD"}
HWLOC_DIR=${HWLOC_DIR:="/sw/hwloc/1.11.5"}
GTG_DIR=${GTG_DIR:="/sw/gtg/0.2-2"}
OMEGA_DIR=${OMEGA_DIR:="/opt/omega"}

. $(dirname $0)/config.inc
guess_defaults
if [[ "x$CC" != "x" && `basename ${CC}` = "icc" ]]; then
  PLASMA_DIR=${PLASMA_DIR:="/sw/plasma/2.8.0i/"}
else
  PLASMA_DIR=${PLASMA_DIR:="/sw/plasma/2.8.0g/"}
fi
TESTCMD="-DMPI_TEST_CMD=\"mpirun -x LD_LIBRARY_PATH -map-by node\" -DSHM_TEST_CMD=\"mpirun -x LD_LIBRARY_PATH -map-by node -np 1\""

run_cmake $*

